{"id":"51774992-2cc6-467d-b81d-cbed57895b7f","revision":0,"last_node_id":46,"last_link_id":70,"nodes":[{"id":5,"type":"EmptyLatentImage","pos":[1845.0916748046875,886.107421875],"size":[315,106],"flags":{},"order":0,"mode":0,"inputs":[],"outputs":[{"localized_name":"LATENT","name":"LATENT","type":"LATENT","slot_index":0,"links":[2]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.27","Node name for S&R":"EmptyLatentImage"},"widgets_values":[1024,1024,1]},{"id":15,"type":"PrimitiveString","pos":[-880.0498657226562,155.7151641845703],"size":[285.6000061035156,58],"flags":{},"order":1,"mode":0,"inputs":[],"outputs":[{"localized_name":"STRING","name":"STRING","type":"STRING","links":[]}],"title":"Provide a context: Chinese culture","properties":{"cnr_id":"comfy-core","ver":"0.3.27","Node name for S&R":"PrimitiveString"},"widgets_values":["Chinese, China, Mandarin, Han-Chinese, Confucianism, Chinese paintings"],"color":"#232","bgcolor":"#353"},{"id":8,"type":"VAEDecode","pos":[1511.0772705078125,901.7804565429688],"size":[210,46],"flags":{},"order":18,"mode":0,"inputs":[{"localized_name":"samples","name":"samples","type":"LATENT","link":7},{"localized_name":"vae","name":"vae","type":"VAE","link":8}],"outputs":[{"localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","slot_index":0,"links":[9]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.27","Node name for S&R":"VAEDecode"},"widgets_values":[]},{"id":7,"type":"CLIPTextEncode","pos":[1382.3824462890625,1219.0897216796875],"size":[425.27801513671875,180.6060791015625],"flags":{},"order":9,"mode":0,"inputs":[{"localized_name":"clip","name":"clip","type":"CLIP","link":5}],"outputs":[{"localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","slot_index":0,"links":[6]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.27","Node name for S&R":"CLIPTextEncode"},"widgets_values":["text, watermark"]},{"id":6,"type":"CLIPTextEncode","pos":[1382.6529541015625,1001.113525390625],"size":[422.84503173828125,164.31304931640625],"flags":{},"order":15,"mode":0,"inputs":[{"localized_name":"clip","name":"clip","type":"CLIP","link":3},{"name":"text","type":"STRING","widget":{"name":"text"},"link":67}],"outputs":[{"localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","slot_index":0,"links":[4]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.27","Node name for S&R":"CLIPTextEncode"},"widgets_values":["beautiful scenery nature glass bottle landscape, , purple galaxy bottle,"]},{"id":4,"type":"CheckpointLoaderSimple","pos":[-184.91697692871094,333.4123229980469],"size":[730.3237915039062,114.64132690429688],"flags":{},"order":2,"mode":0,"inputs":[],"outputs":[{"localized_name":"MODEL","name":"MODEL","type":"MODEL","slot_index":0,"links":[1]},{"localized_name":"CLIP","name":"CLIP","type":"CLIP","slot_index":1,"links":[3,5]},{"localized_name":"VAE","name":"VAE","type":"VAE","slot_index":2,"links":[8]}],"title":"Choose your model (or leave the preset)","properties":{"cnr_id":"comfy-core","ver":"0.3.27","Node name for S&R":"CheckpointLoaderSimple"},"widgets_values":["sd3.5_large_fp8_scaled.safetensors"],"color":"#232","bgcolor":"#353"},{"id":3,"type":"KSampler","pos":[1856.083740234375,1077.40966796875],"size":[352.9214782714844,262],"flags":{},"order":17,"mode":0,"inputs":[{"localized_name":"model","name":"model","type":"MODEL","link":1},{"localized_name":"positive","name":"positive","type":"CONDITIONING","link":4},{"localized_name":"negative","name":"negative","type":"CONDITIONING","link":6},{"localized_name":"latent_image","name":"latent_image","type":"LATENT","link":2}],"outputs":[{"localized_name":"LATENT","name":"LATENT","type":"LATENT","slot_index":0,"links":[7]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.27","Node name for S&R":"KSampler"},"widgets_values":[938422607223119,"randomize",25,7,"euler","normal",1]},{"id":29,"type":"LoadImage","pos":[-213.47055053710938,-554.8912963867188],"size":[315,314],"flags":{},"order":3,"mode":0,"inputs":[],"outputs":[{"localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","links":[57]},{"localized_name":"MASK","name":"MASK","type":"MASK","links":null}],"properties":{"cnr_id":"comfy-core","ver":"0.3.27","Node name for S&R":"LoadImage"},"widgets_values":["IMG_2048.JPG","image",""],"color":"#232","bgcolor":"#353"},{"id":9,"type":"SaveImage","pos":[1231.7845458984375,-859.6812744140625],"size":[782.6429443359375,895.6958618164062],"flags":{},"order":19,"mode":0,"inputs":[{"localized_name":"images","name":"images","type":"IMAGE","link":9}],"outputs":[],"properties":{"cnr_id":"comfy-core","ver":"0.3.27"},"widgets_values":["ComfyUI",""]},{"id":31,"type":"BetterString","pos":[-207.99362182617188,-986.4788208007812],"size":[308.7479553222656,358.1434326171875],"flags":{},"order":4,"mode":0,"inputs":[{"name":"chain","shape":7,"type":"STRING","widget":{"name":"chain"},"link":null}],"outputs":[{"localized_name":"STRING","name":"STRING","type":"STRING","links":[]}],"title":"Input your prompt here","properties":{"cnr_id":"comfyui-better-strings","ver":"1.0.0","Node name for S&R":"BetterString"},"widgets_values":["",""],"color":"#232","bgcolor":"#353"},{"id":38,"type":"Note","pos":[-654.0142211914062,-688.68017578125],"size":[398.78857421875,223.65655517578125],"flags":{},"order":5,"mode":0,"inputs":[],"outputs":[],"title":"Note: USE Prompts OR Image Inputs","properties":{},"widgets_values":["Use either a text prompt or load an image.\n\nMake sure to connect e\n- ither the \"STRING\" output of the prompt module\nor\n- the \"description\" output of tha \"Ollama Vision\" module\nto the \"input_prompt\" connector of the \"AI4ArtsEd Ollama Prompt: Cultural Translation\" Module.\n\nYou may alter the prompt (meta-) instruction in the \"AI4ARtsEd\" Nodes – however, if you do so, please do not save your workflow, but \"save as\" and choose a different name."],"color":"#323","bgcolor":"#535"},{"id":43,"type":"Note","pos":[-601.9805908203125,-946.3348388671875],"size":[297.562255859375,167.02340698242188],"flags":{},"order":6,"mode":0,"inputs":[],"outputs":[],"properties":{},"widgets_values":["IF THINGS DO NOT WORK CORRECTLY:\n\nUPPER RIGHT MENÜ:\n-> Manager -> Restart\n-> Unload Models\n-> Free Model and node cache\n\nTHen:\n-> Reload Webpage in your browser\n"],"color":"#323","bgcolor":"#535"},{"id":45,"type":"ShowText","pos":[643.6464233398438,-828.252685546875],"size":[492.5682678222656,235.56826782226562],"flags":{},"order":14,"mode":0,"inputs":[{"name":"text","type":"STRING","widget":{"name":"text"},"link":69}],"outputs":[{"localized_name":"STRING","name":"STRING","shape":6,"type":"STRING","links":null}],"properties":{"cnr_id":"comfyui-show-text","ver":"1.0.2","Node name for S&R":"ShowText"},"widgets_values":["","```markdown\n| Entity 1   | Relation      | Entity 2     | Relation Type |\n|------------|---------------|--------------|---------------|\n| Cat        | free from     | Restraint    | conceptual    |\n| Cat        | far from      | Dry Land     | spatial       |\n| Cat        | under         | Sand         | spatial       |\n| Cat        | has color     | White        | property      |\n| Claw       | separate from | Cat          | spatial       |\n| Sand       | far from      | Dry Land     | spatial       |\n\n| Subject    | Inferred Relation  | Object / Value   | Type          | Source             |\n|------------|--------------------|------------------|---------------|--------------------|\n| Cat        | appears scared     | Negative Emotion | emotional     | visual cue         |\n| Environment| suggests stress    | Stressful        | environmental | contextual inference|\n```"]},{"id":28,"type":"PrimitiveString","pos":[-201.56451416015625,210.54537963867188],"size":[728.9432373046875,66.43585205078125],"flags":{},"order":7,"mode":0,"inputs":[],"outputs":[{"localized_name":"STRING","name":"STRING","type":"STRING","links":[66]}],"title":"Provide a context: neutral","properties":{"cnr_id":"comfy-core","ver":"0.3.27","Node name for S&R":"PrimitiveString"},"widgets_values":["150 tokens maximum"],"color":"#232","bgcolor":"#353"},{"id":32,"type":"BetterString","pos":[-196.66488647460938,-154.81837463378906],"size":[739.8999633789062,269.59033203125],"flags":{},"order":8,"mode":0,"inputs":[{"name":"chain","shape":7,"type":"STRING","widget":{"name":"chain"},"link":null}],"outputs":[{"localized_name":"STRING","name":"STRING","type":"STRING","links":[60]}],"title":"Describe your style here","properties":{"cnr_id":"comfyui-better-strings","ver":"1.0.0","Node name for S&R":"BetterString"},"widgets_values":["1. Detect all entities and visual elements in the source image.\n2. Transform each element into its diametrical opposite: female → male, old → young, large → small, red → green, joyful → sorrowful, friendly → hostile, etc. Ensure the inversion is categorical and exhaustive.\n3. Identify all inter-entity relations: spatial (above → below, inside → outside), logical (cause → effect), narrative (protagonist → bystander), and kinship (mother → son or stranger). Invert each relation accordingly.\n4. Reconstruct a coherent image concept using the transformed entities in their reversed relations. Preserve the structural logic of the original while applying full semantic reversal.\n5. Output only the final configuration. Do not include commentary, explanations, or meta-descriptions.",""],"color":"#232","bgcolor":"#353"},{"id":40,"type":"ai4artsed_openrouter","pos":[644.0596313476562,-519.0744018554688],"size":[491.4000244140625,268],"flags":{},"order":11,"mode":0,"inputs":[{"name":"input_prompt","type":"STRING","widget":{"name":"input_prompt"},"link":59},{"name":"input_context","type":"STRING","widget":{"name":"input_context"},"link":60}],"outputs":[{"localized_name":"output","name":"output","type":"STRING","links":[65,69]}],"properties":{"aux_id":"joeriben/ai4artsed_comfyui","ver":"0464d155e31fa8ab956aa907e233794cfb04bc70","Node name for S&R":"ai4artsed_openrouter"},"widgets_values":["","","Transform the image description according to the logic and instructions defined in the input labeled “input_context”.\nThis is not a linguistic translation, but a semantic and structural transformation.\nReconstruct all entities and their relations as specified, ensuring that:\n- Each entity is retained and transformed as instructed.\n- Each relation is inverted or altered in line with the logic of the “Context”.\n- The resulting structure reflects a complete semantic inversion.\nDo not explain your reasoning.\nOutput only the transformed description, either as structured data (if input was structured), or as plain descriptive text (if input was textual).","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","google/gemini-2.5-pro-preview-03-25","enable"],"color":"#322","bgcolor":"#533"},{"id":39,"type":"ai4artsed_openrouter_imageanalysis","pos":[140.90109252929688,-504.95263671875],"size":[429.23907470703125,278.0320739746094],"flags":{},"order":10,"mode":0,"inputs":[{"localized_name":"images","name":"images","type":"IMAGE","link":57}],"outputs":[{"localized_name":"response","name":"response","type":"STRING","links":[59,68]}],"properties":{"aux_id":"joeriben/ai4artsed_comfyui","ver":"0464d155e31fa8ab956aa907e233794cfb04bc70","Node name for S&R":"ai4artsed_openrouter_imageanalysis"},"widgets_values":["Analyze the image and output exactly two separate tables. Do not include any introductory or explanatory text beyond these tables.\nFirst Table: Visually Observable Entities and Relationships\nFormat:\n| Entity 1 | Relation | Entity 2 | Relation Type |\nInstructions:\n1. Identify all visible or implied entities (people, objects, animals, symbols, elements of the environment, etc.).\n2. For each pair of entities, specify a concrete relation that connects them using clear, unambiguous terms (e.g., \"above\", \"next to\", \"inside\", \"wearing\", \"has color\").\n3. Classify each relation into one of the following types:\n- spatial (e.g., above, next to, inside)\n- temporal (e.g., before, after, simultaneous)\n- causal (e.g., causes, enables, prevents)\n- symbolic (e.g., represents, connotes, expresses)\n- narrative (e.g., protagonist of, is acted upon by, interacts with)\n- social (e.g., mother of, enemy of, supports)\n- property (e.g., has color, has gender, has age, shows emotion)\nSecond Table: Inferred or Interpretative Semantic Relations\nFormat:\n| Subject | Inferred Relation | Object / Value | Type | Source |\nInstructions:\n1. Extract higher-level, inferred semantic relations based on visual cues and contextual hints. These relations can include emotional states, narrative context, symbolic meanings, social dynamics, environmental impressions, temporal cues, or attentional focus.\n2. For each inference, specify the relation clearly and classify it using one of the following types: emotional, narrative, symbolic, social, environmental, temporal, attentional, or other.\n3. In the “Source” column, indicate the origin of the inference (e.g., \"visual cue\", \"contextual inference\", \"cultural trope\", \"associative logic\").\n4. Use one row per relation.\nOutput only the two markdown tables in the specified format.","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","openai/gpt-4o",1024,0.7],"color":"#322","bgcolor":"#533"},{"id":44,"type":"ShowText","pos":[135.48101806640625,-826.9487915039062],"size":[474.8935852050781,251.2535858154297],"flags":{},"order":12,"mode":0,"inputs":[{"name":"text","type":"STRING","widget":{"name":"text"},"link":68}],"outputs":[{"localized_name":"STRING","name":"STRING","shape":6,"type":"STRING","links":null}],"properties":{"cnr_id":"comfyui-show-text","ver":"1.0.2","Node name for S&R":"ShowText"},"widgets_values":["","```markdown\n| Entity 1   | Relation   | Entity 2     | Relation Type |\n|------------|------------|--------------|---------------|\n| Dog        | wearing    | Harness      | spatial       |\n| Dog        | near       | Water        | spatial       |\n| Dog        | on         | Grass        | spatial       |\n| Dog        | has color  | Brown        | property      |\n| Tongue     | part of    | Dog          | spatial       |\n| Grass      | next to    | Water        | spatial       |\n\n| Subject    | Inferred Relation  | Object / Value | Type       | Source             |\n|------------|--------------------|----------------|------------|--------------------|\n| Dog        | appears happy      | Positive Emotion | emotional  | visual cue         |\n| Environment| suggests relaxation| Calm           | environmental| contextual inference|\n```"]},{"id":46,"type":"ShowText","pos":[669.3324584960938,263.9596862792969],"size":[466.8822937011719,329.3779296875],"flags":{},"order":16,"mode":0,"inputs":[{"name":"text","type":"STRING","widget":{"name":"text"},"link":70}],"outputs":[{"localized_name":"STRING","name":"STRING","shape":6,"type":"STRING","links":null}],"properties":{"cnr_id":"comfyui-show-text","ver":"1.0.2","Node name for S&R":"ShowText"},"widgets_values":["","Panicked white cat partially buried in dark wet sand, vast stormy sea background, far from land, wide terrified eyes, ruffled fur, dramatic chiaroscuro lighting, intense atmosphere, close-up shot, a single detached claw rests on sand nearby."]},{"id":42,"type":"ai4artsed_openrouter","pos":[652.978271484375,-157.1272430419922],"size":[487.6839294433594,334.8896179199219],"flags":{},"order":13,"mode":0,"inputs":[{"name":"input_prompt","type":"STRING","widget":{"name":"input_prompt"},"link":65},{"name":"input_context","type":"STRING","widget":{"name":"input_context"},"link":66}],"outputs":[{"localized_name":"output","name":"output","type":"STRING","links":[67,70]}],"properties":{"aux_id":"joeriben/ai4artsed_comfyui","ver":"0464d155e31fa8ab956aa907e233794cfb04bc70","Node name for S&R":"ai4artsed_openrouter"},"widgets_values":["","","Use the input to create a prompt suitable for Stable Diffusion 3.5.\nInstructions:\n1. Select only information that can be rendered visually: entities, their visual properties, spatial configurations, symbolic or narrative roles if they have visual consequences (e.g. pose, gesture, setting, costume).\n2. Ignore non-visual symbolic or abstract relations unless they manifest as visible differences (e.g. \"freedom\" → \"confinement\" becomes \"open field\" → \"prison room\").\n3. Condense the result into a comma-separated prompt with no more than 75 tokens. Use compressed, stylized language typical for SD prompts.\n4. Do not use full sentences. Avoid repetition. Do not include any meta-text or explanation.\n5. Emphasize key compositional and stylistic features if evident (e.g. centered, dramatic lighting, overcast, wide angle, cinematic).\nOutput: Only the resulting prompt as plain text. No table, no explanation, no formatting.\n\nIf the resulting prompt is clearly under 75 tokens, expand it by:\n– adding visually descriptive adjectives (e.g. lighting, texture, motion, color quality)\n– specifying environmental or background elements that imply the mood\n– including camera or compositional hints (e.g. angle, framing, focus)\n– inserting emotional or symbolic indicators only if they have visual impact\n\nDo not pad artificially or repeat. Prioritize meaningful, image-relevant details.","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","google/gemini-2.5-pro-preview-03-25","enable"],"color":"#322","bgcolor":"#533"}],"links":[[1,4,0,3,0,"MODEL"],[2,5,0,3,3,"LATENT"],[3,4,1,6,0,"CLIP"],[4,6,0,3,1,"CONDITIONING"],[5,4,1,7,0,"CLIP"],[6,7,0,3,2,"CONDITIONING"],[7,3,0,8,0,"LATENT"],[8,4,2,8,1,"VAE"],[9,8,0,9,0,"IMAGE"],[57,29,0,39,0,"IMAGE"],[59,39,0,40,0,"STRING"],[60,32,0,40,1,"STRING"],[65,40,0,42,0,"STRING"],[66,28,0,42,1,"STRING"],[67,42,0,6,1,"STRING"],[68,39,0,44,0,"STRING"],[69,40,0,45,0,"STRING"],[70,42,0,46,0,"STRING"]],"groups":[],"config":{},"extra":{"ds":{"scale":1.1918176537727234,"offset":[-331.79303490744985,136.40867711839644]},"linkExtensions":[{"id":19,"parentId":1}]},"version":0.4}